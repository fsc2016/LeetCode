{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:23:52.686564Z",
     "start_time": "2020-10-26T12:23:52.548654Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from typing import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import  IntegerType,ArrayType,StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec,Word2VecModel\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:24:27.601302Z",
     "start_time": "2020-10-26T12:24:24.111811Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('enbbeding').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:24:44.895707Z",
     "start_time": "2020-10-26T12:24:43.656812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sentence: array<string>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = (\"a b \" * 100 + \"a c \" * 10).split(\" \")\n",
    "doc = spark.createDataFrame([(sent,), (sent,)], [\"sentence\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:25:03.623806Z",
     "start_time": "2020-10-26T12:24:56.435315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            sentence|\n",
      "+--------------------+\n",
      "|[a, b, a, b, a, b...|\n",
      "|[a, b, a, b, a, b...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:42:08.573225Z",
     "start_time": "2020-10-26T12:41:55.500298Z"
    }
   },
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"sentence\")\n",
    "model = word2Vec.fit(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:42:55.644782Z",
     "start_time": "2020-10-26T12:42:55.604062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|word|              vector|\n",
      "+----+--------------------+\n",
      "|   a|[0.09461779892444...|\n",
      "|   b|[1.15474212169647...|\n",
      "|   c|[-0.3794820010662...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:42:57.045771Z",
     "start_time": "2020-10-26T12:42:57.033777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T12:43:19.987989Z",
     "start_time": "2020-10-26T12:43:19.983998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.Word2VecModel"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T13:20:50.585267Z",
     "start_time": "2020-10-26T13:20:50.577761Z"
    }
   },
   "outputs": [],
   "source": [
    "df=model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T13:26:12.964299Z",
     "start_time": "2020-10-26T13:26:12.941281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(word='b', vector=DenseVector([1.1547, -0.5933, -0.8722, 0.4669, 0.5515]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i,j in df.\n",
    "df.collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T13:26:17.400164Z",
     "start_time": "2020-10-26T13:26:17.365664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0.0946177989244461,-0.4951631426811218,0.06406556069850922,-0.37930983304977417,0.21593928337097168]\n",
      "b [1.1547421216964722,-0.593326210975647,-0.8721810579299927,0.4669361710548401,0.551497220993042]\n",
      "c [-0.3794820010662079,0.34077689051628113,0.06388652324676514,0.0352821946144104,-0.24136029183864594]\n"
     ]
    }
   ],
   "source": [
    "for row in df.collect():\n",
    "    print(row['word'],row['vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:35:12.634199Z",
     "start_time": "2020-11-02T14:35:12.593726Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from typing import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import  IntegerType,ArrayType,StringType\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import *\n",
    "import os,random\n",
    "from collections import defaultdict\n",
    "import redis\n",
    "\n",
    "HOST ='localhost'\n",
    "PORT = 6379\n",
    "def sortByTime(movieid_time_list:List):\n",
    "    '''\n",
    "    按照时间戳排序，返回movieids\n",
    "    :param movieid_time_list:\n",
    "    :return:\n",
    "    '''\n",
    "    movieid_time_list.sort(key=lambda x:x[1])\n",
    "    mids = [i[0] for i in movieid_time_list]\n",
    "    return mids\n",
    "\n",
    "def processItemSequence(spark:SparkSession):\n",
    "    '''\n",
    "    处理评分数据，筛选评分大于3.5的，按照用户id分组获取评分电影序列\n",
    "    :param spark:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    df = spark.read.format('csv').option('header', 'true').load('./data/ratings.csv')\n",
    "    df.printSchema()\n",
    "\n",
    "    sortUdf = udf(f=sortByTime,returnType=ArrayType(StringType()))\n",
    "    userSeq=df.where(df['rating'] >= 3.5).groupby('userId').agg(sortUdf(collect_list(struct('movieId','timestamp'))).alias('movieIds'))\\\n",
    "        .withColumn('movieIdStr',array_join('movieIds',' '))\n",
    "    userSeq.show(5)\n",
    "\n",
    "    #不使用udf，速度慢一点\n",
    "    # userSeq=df.where(df['rating'] >= 3.5).sort('timestamp').groupby('userId').agg(collect_list('movieId').alias('movieIds')).withColumn('movieIdStr',array_join('movieIds',' ')).show(10)\n",
    "\n",
    "    userSeq.printSchema()\n",
    "    dataset = userSeq.select('movieIds')\n",
    "    moviesCount = dataset.select(explode(col('movieIds'))).alias('tmp').distinct().count()\n",
    "    print('unique high rating movies:{}'.format(moviesCount))\n",
    "    # print(dataset.count())\n",
    "    return dataset\n",
    "\n",
    "def trainItem2vec(dataset,filename,saveToRedis=False,redisKeyPrefix=None):\n",
    "    '''\n",
    "    训练产生embedding,inputCol需要是 array（string）类型\n",
    "    训练好后写入 filename\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    '''\n",
    "    word2vec = Word2Vec(vectorSize=10,windowSize=5,maxIter=10,inputCol='movieIds')\n",
    "    model = word2vec.fit(dataset)\n",
    "    print('model fitted')\n",
    "    # 打印相似电影，基于点积运算\n",
    "    synonyms = model.findSynonymsArray('158',20)\n",
    "    for moveid,similarity in synonyms:\n",
    "        print('{}:{}'.format(moveid,similarity))\n",
    "\n",
    "    with open('./modeldata/{}'.format(filename),'w') as f:\n",
    "        for row in model.getVectors().collect():\n",
    "            tmp=','.join([str(vector) for vector in row['vector']])\n",
    "            f.write('{}:{}\\n'.format(row['word'],tmp))\n",
    "\n",
    "    # redis-cli eval \"redis.call('del', unpack(redis.call('keys','*')))\" 0 windows批量删除key\n",
    "    if saveToRedis:\n",
    "        pool = redis.ConnectionPool(host=HOST,port=PORT)\n",
    "        # key的存活时间 秒\n",
    "        ex = 60 * 10\n",
    "        r = redis.Redis(connection_pool=pool)\n",
    "        for i,row in enumerate(model.getVectors().collect()):\n",
    "            tmp = ','.join([str(vector) for vector in row['vector']])\n",
    "            if i == 1:\n",
    "                print(type(row['vector']))\n",
    "            r.set('{}:{}'.format(redisKeyPrefix,row['word']),tmp,ex)\n",
    "    return model \n",
    "\n",
    "\n",
    "def dealPairMovie(movies:Row)->List:\n",
    "    '''\n",
    "    udf\n",
    "    :param movies:\n",
    "    :return:\n",
    "    '''\n",
    "    newl=[]\n",
    "    movies = movies['movieIds']\n",
    "    for i in range(len(movies)-1):\n",
    "        newl.append((movies[i],movies[i+1]))\n",
    "    return newl\n",
    "\n",
    "\n",
    "def generateTransitionMatrix(dataset:DataFrame):\n",
    "    '''\n",
    "    生成状态转移矩阵\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    '''\n",
    "    pairSamples=dataset.rdd.flatMap(dealPairMovie)\n",
    "    pairSamples.cache()\n",
    "    print(pairSamples.take(10))\n",
    "    print('pairSamples over')\n",
    "    # {(mid,mid2):count,...}\n",
    "\n",
    "    pairCountMap = pairSamples.countByValue()\n",
    "\n",
    "    print('pairCountMap_{}'.format(len(pairCountMap)))\n",
    "    # 计数状态矩阵\n",
    "    transitionCountMatrix = defaultdict(dict)\n",
    "    itemCountMap = defaultdict(int)\n",
    "    all_count=0\n",
    "    for k,count in pairCountMap.items():\n",
    "        transitionCountMatrix[k[0]][k[1]] = count\n",
    "        itemCountMap[k[0]] +=count\n",
    "        all_count+=count\n",
    "    print('transitionCountMatrix over')\n",
    "    #概率状态矩阵\n",
    "    transitionMatrix = defaultdict(dict)\n",
    "    itemDistribution = defaultdict(int)\n",
    "    for a,cmap in transitionCountMatrix.items():\n",
    "        for b,count in cmap.items():\n",
    "            transitionMatrix[a][b] = float(count /itemCountMap[a])\n",
    "\n",
    "    for k,count in itemCountMap.items():\n",
    "        itemDistribution[k] = float(count / all_count)\n",
    "\n",
    "    print('transitionMatrix_{}'.format(len(transitionMatrix)))\n",
    "    print(transitionMatrix['858'])\n",
    "    print('itemDistribution_{}'.format(len(itemDistribution)))\n",
    "    print(itemDistribution['858'])\n",
    "    return transitionMatrix,itemDistribution\n",
    "\n",
    "def oneRandomWalk(transitionMatrix, itemDistribution, sampleLength):\n",
    "    '''\n",
    "    单次随机游走\n",
    "    :param transitionMatrix:\n",
    "    :param itemDistribution:\n",
    "    :param sampleLength:\n",
    "    :return:\n",
    "    '''\n",
    "    sample = []\n",
    "    randomValue = random.random()\n",
    "    firstItem=''\n",
    "    accumulateProb=0\n",
    "\n",
    "    # 按照电影分布，取第一部电影\n",
    "    for k,v in itemDistribution.items():\n",
    "        accumulateProb+=v\n",
    "        if accumulateProb >= randomValue:\n",
    "            firstItem=k\n",
    "            break\n",
    "    sample.append(firstItem)\n",
    "    curItem = firstItem\n",
    "\n",
    "    # 按照状态转移，取后面9部电影\n",
    "    for i in range(1,sampleLength):\n",
    "        if not transitionMatrix[curItem] or not itemDistribution[curItem]:\n",
    "            break\n",
    "        # 随机游走的策略\n",
    "        randomProb = random.random()\n",
    "        for k, prob in transitionMatrix[curItem].items():\n",
    "            if randomProb >= prob:\n",
    "                curItem = k\n",
    "                break\n",
    "\n",
    "        sample.append(curItem)\n",
    "    return sample\n",
    "\n",
    "def randomWalk(transitionMatrix,itemDistribution,sampleCount,sampleLength):\n",
    "    '''\n",
    "    随机游走\n",
    "    :param transitionMatrix:\n",
    "    :param itemDistribution:\n",
    "    :param sampleCount:\n",
    "    :param sampleLength:\n",
    "    :return:\n",
    "    '''\n",
    "    samples = []\n",
    "    for i in range(sampleCount):\n",
    "        samples.append(oneRandomWalk(transitionMatrix, itemDistribution, sampleLength))\n",
    "    return samples\n",
    "\n",
    "def oneNode2vec(transitionMatrix, itemDistribution, sampleLength):\n",
    "\n",
    "    p , q  = 0.1, 0.2\n",
    "    sample = []\n",
    "    randomValue = random.random()\n",
    "    firstItem = ''\n",
    "    accumulateProb = 0\n",
    "\n",
    "    # 按照电影分布，取第一部电影\n",
    "    for k, v in itemDistribution.items():\n",
    "        accumulateProb += v\n",
    "        if accumulateProb >= randomValue:\n",
    "            firstItem = k\n",
    "            break\n",
    "\n",
    "    sample.append(firstItem)\n",
    "    curItem = firstItem\n",
    "    #nodeT始终是curElement的前一个值\n",
    "    nodeT = curItem\n",
    "    # 按照状态转移，取后面9部电影\n",
    "    for i in range(1, sampleLength):\n",
    "        if not transitionMatrix[curItem] or not itemDistribution[curItem]:\n",
    "            break\n",
    "        randomProb = random.random()\n",
    "        # 第一步时，curItem和nodeT是同一个点，所以要保持nodeT不动，curIte前进一步\n",
    "        if i == 1:\n",
    "            for item, prob in transitionMatrix[curItem].items():\n",
    "                if randomProb >= prob:\n",
    "                    curItem = item\n",
    "                    break\n",
    "        else:\n",
    "            for item, prob in transitionMatrix[curItem].items():\n",
    "                # 跳回前一节点\n",
    "                if item == nodeT:\n",
    "                    prob = prob * 1 / p\n",
    "                #distince =1\n",
    "                elif item in transitionMatrix[nodeT]:\n",
    "                    prob = prob\n",
    "                #distince =2\n",
    "                else:\n",
    "                    prob = prob * 1/q\n",
    "\n",
    "                if randomProb >= prob:\n",
    "                    nodeT = curItem\n",
    "                    curItem = item\n",
    "                    break\n",
    "        sample.append(curItem)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def node2vec(transitionMatrix,itemDistribution,sampleCount,sampleLength):\n",
    "    samples = []\n",
    "    for i in range(sampleCount):\n",
    "        samples.append(oneNode2vec(transitionMatrix, itemDistribution, sampleLength))\n",
    "    return samples\n",
    "\n",
    "def graphEmb(dataset:DataFrame,spark:SparkSession,embOutputFilename,saveToRedis=False,redisKeyPrefix=None):\n",
    "    '''\n",
    "    图enbding\n",
    "    :param dataset:\n",
    "    :param spark:\n",
    "    :param embOutputFilename:\n",
    "    :return:\n",
    "    '''\n",
    "    transitionMatrix, itemDistribution=generateTransitionMatrix(dataset)\n",
    "    sampleCount = 20000\n",
    "    sampleLength = 10\n",
    "\n",
    "    # newSamples=randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength)\n",
    "    newSamples = node2vec(transitionMatrix, itemDistribution, sampleCount, sampleLength)\n",
    "\n",
    "    # 转为rdd\n",
    "    rddSamples=spark.sparkContext.parallelize([Row(movieIds=i) for i in newSamples])\n",
    "    print(newSamples[:10])\n",
    "    print(rddSamples.take(10))\n",
    "    # 转为DataFrame\n",
    "    dataFrameSamples = spark.createDataFrame(rddSamples)\n",
    "    print(type(dataFrameSamples))\n",
    "    print(dataFrameSamples.take(10))\n",
    "    # trainItem2vec(dataFrameSamples,embOutputFilename,saveToRedis,redisKeyPrefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:50:09.297980Z",
     "start_time": "2020-11-02T13:50:03.678073Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('enbbeding').master('local[*]').getOrCreate()\n",
    "df = spark.read.format('csv').option('header', 'true').load('./data/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:57:41.100515Z",
     "start_time": "2020-11-02T13:57:39.421044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|lengthMov|\n",
      "+---------+\n",
      "|    29776|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "um=df.groupBy('userId').agg(collect_list(col('movieId')).alias('movieIds'))\n",
    "um.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:39:56.989451Z",
     "start_time": "2020-11-02T14:35:21.410078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+--------------------+--------------------+\n",
      "|userId|            movieIds|          movieIdStr|\n",
      "+------+--------------------+--------------------+\n",
      "| 10096| [858, 50, 593, 457]|      858 50 593 457|\n",
      "| 10351|[1, 25, 32, 6, 60...|1 25 32 6 608 52 ...|\n",
      "| 10436|[661, 107, 60, 1,...|661 107 60 1 919 ...|\n",
      "|  1090|[356, 597, 919, 986]|     356 597 919 986|\n",
      "| 11078|[232, 20, 296, 59...|232 20 296 593 45...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieIds: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- movieIdStr: string (nullable = true)\n",
      "\n",
      "unique high rating movies:959\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "model fitted\n",
      "48:0.9619194865226746\n",
      "256:0.9318743348121643\n",
      "31:0.9131985902786255\n",
      "186:0.9042068719863892\n",
      "355:0.8738464713096619\n",
      "168:0.8677228093147278\n",
      "520:0.8554422855377197\n",
      "277:0.8438514471054077\n",
      "252:0.8363022804260254\n",
      "552:0.8346211910247803\n",
      "276:0.8223102688789368\n",
      "432:0.8209726810455322\n",
      "44:0.8203796148300171\n",
      "2:0.7853690385818481\n",
      "236:0.781509280204773\n",
      "455:0.7765454053878784\n",
      "333:0.7729570269584656\n",
      "169:0.767143189907074\n",
      "362:0.752589762210846\n",
      "370:0.7395660877227783\n",
      "<class 'pyspark.ml.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "dataset=processItemSequence(spark)\n",
    "print(type(dataset))\n",
    "model=trainItem2vec(dataset,'item2vecEmb1.txt',saveToRedis=True,redisKeyPrefix='i2vEmb')\n",
    "rows =model.getVectors().collect()\n",
    "movdict={}\n",
    "for row in rows:\n",
    "    movdict[row['word']] = list(row['vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:51:53.629941Z",
     "start_time": "2020-11-02T14:51:53.601942Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:51:56.303779Z",
     "start_time": "2020-11-02T14:51:56.174223Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:53:13.924816Z",
     "start_time": "2020-11-02T13:53:12.283078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29776"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def(movdict,movieids:Row):\n",
    "    useremb=[0] *10\n",
    "    movies = movies['movieIds']\n",
    "    for movieid in movies:\n",
    "        movEmb = movdict.get('movieid')\n",
    "        if movEmb:\n",
    "            useremb=[useremb[i]+movEmb[i] for i in range(10)]\n",
    "    return useremb\n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
